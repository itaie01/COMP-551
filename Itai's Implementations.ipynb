{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this jupyter notebook contains my implementations of work from COMP 551: Applied Machine Learning at McGill University. It contains a decision tree, multilayer perceptron, and textual data analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the gini index/impurity classification cost\n",
    "def gini_index(classes):\n",
    "    class_probabilities = (\n",
    "        np.bincount(classes) / classes.shape[0]\n",
    "    )  # count the number of each class and divide it by the total number of classes (calculate class probabilities/proportions)\n",
    "    return 1 - np.sum(np.square(class_probabilities))\n",
    "\n",
    "\n",
    "# calculate the entropy classification cost\n",
    "def entropy(classes):\n",
    "    class_probabilities = (\n",
    "        np.bincount(classes) / classes.shape[0]\n",
    "    )  # calculate the class probabilities\n",
    "    non_zero_indices = np.nonzero(\n",
    "        class_probabilities\n",
    "    )  # remove all probabilities that are 0 to prevent error with log(0)\n",
    "    return -np.sum(\n",
    "        class_probabilities[non_zero_indices]\n",
    "        * np.log2(class_probabilities[non_zero_indices])\n",
    "    )  # calculate the entropy\n",
    "\n",
    "\n",
    "def misclassification(classes):\n",
    "    class_probabilities = np.bincount(classes) / classes.shape[0]\n",
    "    return 1 - np.max(class_probabilities)\n",
    "\n",
    "\n",
    "class Node:  # assume it is a leaf, else add parent information\n",
    "    def __init__(self, data_indices, parent):\n",
    "        self.left_node = None  # store the left node of the parent node\n",
    "        self.right_node = None  # store the right node of the parent node\n",
    "        self.data_indices = data_indices  # store the data indices to access the data instances under/associated with this node\n",
    "        self.split_feature = None  # store the split feature for this node\n",
    "        self.split_threshold = (\n",
    "            None  # store the split threshold/value of the feature for this node\n",
    "        )\n",
    "        if parent:\n",
    "            self.depth = parent.depth + 1  # store the depth and increase it by 1\n",
    "            self.num_classes = (\n",
    "                parent.num_classes\n",
    "            )  # store the number of classes for this node\n",
    "\n",
    "\n",
    "class DecisionTree:  # Adjust how the num_classes works\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes=None,\n",
    "        max_depth=10,\n",
    "        min_leaf_instances=1,\n",
    "        cost_function=gini_index,\n",
    "    ):\n",
    "        self.root = None  # initialize the root as empty\n",
    "        self.num_classes = int(num_classes)  # store the number of classes parameter\n",
    "        self.max_depth = max_depth  # store the max depth paramater\n",
    "        self.cost_function = cost_function  # store the cost function to be used\n",
    "        self.min_leaf_instances = (\n",
    "            min_leaf_instances  # store the mininum leaf instances parameter\n",
    "        )\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X = X  # store the data\n",
    "        self.y = y.astype(int)  # store the labels/classes\n",
    "        self.root = Node(\n",
    "            data_indices=np.arange(X.shape[0]), parent=None\n",
    "        )  # initialize the root node with necessary data indices\n",
    "        self.root.depth = 0  # initialize the root depth at 0\n",
    "        self.root.num_classes = self.num_classes  # store the number of classes\n",
    "        class_probabilities = np.bincount(\n",
    "            self.y[self.root.data_indices], minlength=self.num_classes\n",
    "        )  # calculate the class probabilities\n",
    "        self.root.class_probabilities = class_probabilities / np.sum(\n",
    "            class_probabilities\n",
    "        )  # store the class probabiltiies in the root\n",
    "        self.__fit_tree(self.root)  # build the decision tree from the input data\n",
    "        return self\n",
    "\n",
    "    # greedy test algorithm derived from the class slides and Decision Tree CoLab example\n",
    "    def __greedy_test(self, node):\n",
    "        best_cost = np.inf  # set the best cost to the maximum possible value\n",
    "        best_feature = None  # set the split feature to None\n",
    "        best_threshold = None  # set the split threshold/value to None\n",
    "        num_instances, num_features = self.X[\n",
    "            node.data_indices\n",
    "        ].shape  # store the number of data instances and the amount of features of the dataset\n",
    "\n",
    "        for feature in range(num_features):  # loop through each feature\n",
    "            feature_values = self.X[\n",
    "                node.data_indices, feature\n",
    "            ]  # store the values/instances associated with this node for this feature\n",
    "            unique_instances = np.unique(\n",
    "                feature_values\n",
    "            )  # get the unique values for this feature\n",
    "            if len(unique_instances > 1):  # check if the number of unique values > 1\n",
    "                test_thresholds = (\n",
    "                    unique_instances[0:-1] + unique_instances[1:]\n",
    "                ) / 2  # take the average of consecutive unique values\n",
    "            else:\n",
    "                test_thresholds = unique_instances  # if there is only one unique value set it as the test threshold\n",
    "\n",
    "            for test_threshold in unique_instances:  # loop through each test threshold\n",
    "                left_indices = node.data_indices[\n",
    "                    feature_values <= test_threshold\n",
    "                ]  # store the data indices where the associated feature instance <= test_threshold (boolean array indexing)\n",
    "                right_indices = node.data_indices[\n",
    "                    feature_values > test_threshold\n",
    "                ]  # store the data indices where the associated feature instance > test_threshold (boolean array indexing)\n",
    "\n",
    "                num_left = len(left_indices)  # count the number of left node indices\n",
    "                num_right = len(right_indices)  # count the number of right node indices\n",
    "\n",
    "                if (\n",
    "                    num_left == 0 or num_right == 0\n",
    "                ):  # if either is empty, skip this value, there's no extra node to add at this threshold\n",
    "                    continue\n",
    "\n",
    "                cost_left = self.cost_function(\n",
    "                    self.y[left_indices]\n",
    "                )  # calculate the cost for the classes associated with the left data split\n",
    "                cost_right = self.cost_function(\n",
    "                    self.y[right_indices]\n",
    "                )  # calculate the cost for the classes associated with the right data split\n",
    "\n",
    "                total_cost = (\n",
    "                    ((num_left * cost_left) + (num_right * cost_right)) / num_instances\n",
    "                )  # calculate the total cost with apporpriate \"weight\" given to the left and right indices respectively\n",
    "\n",
    "                # if the calculated total cost is less than the best cost, store the new values/indices associated with it\n",
    "                if total_cost < best_cost:\n",
    "                    best_cost = total_cost\n",
    "                    best_feature = feature\n",
    "                    best_threshold = test_threshold\n",
    "\n",
    "        return best_cost, best_feature, best_threshold\n",
    "\n",
    "    def __fit_tree(self, node):\n",
    "        # if we've reached the max depth or the amount of data indices at this node are <= min leaf instances, then stop building the tree\n",
    "        if (\n",
    "            node.depth == self.max_depth\n",
    "            or len(node.data_indices) <= self.min_leaf_instances\n",
    "        ):\n",
    "            return\n",
    "\n",
    "        # find the split cost, split feature, and split threshold value foer this node\n",
    "        best_cost, best_feature, best_threshold = self.__greedy_test(node)\n",
    "\n",
    "        # if the cost is still the max value, stop\n",
    "        if np.isinf(best_cost):\n",
    "            return\n",
    "\n",
    "        # find all data instances <= split threshold\n",
    "        test_logicals_1 = self.X[node.data_indices, best_feature] <= best_threshold\n",
    "        # find all data instances > split threshold\n",
    "        test_logicals_2 = self.X[node.data_indices, best_feature] > best_threshold\n",
    "\n",
    "        node.split_feature = best_feature  # store the split feature for this node\n",
    "        node.split_threshold = best_threshold  # store the split value for this node\n",
    "\n",
    "        # innitalize the left node for this node\n",
    "        left_node = Node(\n",
    "            data_indices=node.data_indices[test_logicals_1], parent=node\n",
    "        )  # filter the data indices based on the corresponding data indices <= split value\n",
    "        class_probabilities = np.bincount(\n",
    "            self.y[left_node.data_indices], minlength=self.num_classes\n",
    "        )\n",
    "        left_node.class_probabilities = class_probabilities / np.sum(\n",
    "            class_probabilities\n",
    "        )  # calculate the class probabilities for the left node and store it\n",
    "\n",
    "        # innitalize the right node for this node\n",
    "        right_node = Node(\n",
    "            data_indices=node.data_indices[test_logicals_2], parent=node\n",
    "        )  # filter the data indices based on the corresponding data indices > split value\n",
    "        num_classes_right = np.unique(self.y[right_node.data_indices])\n",
    "        class_probabilities = np.bincount(\n",
    "            self.y[right_node.data_indices], minlength=self.num_classes\n",
    "        )\n",
    "        right_node.class_probabilities = class_probabilities / np.sum(\n",
    "            class_probabilities\n",
    "        )  # calculate the class probabilities for the right node and store it\n",
    "\n",
    "        self.__fit_tree(left_node)  # continue to build the tree with the left node\n",
    "        self.__fit_tree(right_node)  # continue to build the tree with the right node\n",
    "\n",
    "        node.left_node = (\n",
    "            left_node  # store the newly initialized left node as this node's left node\n",
    "        )\n",
    "        node.right_node = right_node  # store the newly initialized right node as this node's right node\n",
    "\n",
    "    def predict(self, new_data):\n",
    "        counter = 0  # initialize counter as 0\n",
    "        class_probabilities = np.zeros((new_data.shape[0], self.num_classes))\n",
    "        for datum in new_data:  # get each new data instance we want to classify\n",
    "            node = self.root  # set the current node to the tree's root\n",
    "\n",
    "            while node.left_node:  # white there is a left node to explore\n",
    "                if (\n",
    "                    datum[node.split_feature] <= node.split_threshold\n",
    "                ):  # if the value of the new data at the split feature < split threshold\n",
    "                    node = node.left_node  # then set the current node to the left node\n",
    "                else:\n",
    "                    node = (\n",
    "                        node.right_node\n",
    "                    )  # else set the current node to the right node\n",
    "\n",
    "            # once a leaf/max depth has been reached set the class probabilities for the new data instance as the class probabilities associated with the leaf node\n",
    "            class_probabilities[counter, :] = node.class_probabilities\n",
    "            counter += 1  # increment counter by 1\n",
    "        return class_probabilities\n",
    "\n",
    "    # evaluate the accuracy of the predicted values\n",
    "    def evaluate_accuracy(self, y_test, y_real):\n",
    "        return np.sum(y_test == y_real) / y_real.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base classes\n",
    "class NeuralNetLayer:\n",
    "    def __init__(self):\n",
    "        self.gradient = None\n",
    "        self.parameters = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "# Linear Calculations x.T dot w + b\n",
    "class LinearLayer(NeuralNetLayer):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.ni = input_size\n",
    "        self.no = output_size\n",
    "        self.w = np.random.randn(output_size, input_size)\n",
    "        self.b = np.random.randn(output_size)\n",
    "        self.cur_input = None\n",
    "        self.parameters = [self.w, self.b]\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.cur_input = x\n",
    "        return (self.w[None, :, :] @ x[:, :, None]).squeeze() + self.b\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        assert self.cur_input is not None, \"Must call forward before backward\"\n",
    "        # dw = gradient.dot(self.cur_input)\n",
    "        dw = gradient[:, :, None] @ self.cur_input[:, None, :]\n",
    "        db = gradient\n",
    "        self.gradient = [dw, db]\n",
    "        return gradient.dot(self.w)\n",
    "\n",
    "\n",
    "# ReLU activation function\n",
    "class ReLULayer(NeuralNetLayer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.gradient = np.where(x > 0, 1.0, 0.0)\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        assert self.gradient is not None, \"Must call forward before backward\"\n",
    "        return gradient * self.gradient\n",
    "\n",
    "\n",
    "# Tanh activation function\n",
    "class TanhLayer(NeuralNetLayer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        tanh = lambda a: (np.exp(a) - np.exp(-a)) / (np.exp(a) + np.exp(-a))\n",
    "        self.gradient = 1 - np.square(tanh(x))\n",
    "        return tanh(x)\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        assert self.gradient is not None, \"Must call forward before backward\"\n",
    "        return gradient * self.gradient\n",
    "\n",
    "\n",
    "class LeakyReLULayer(NeuralNetLayer):\n",
    "    def __init__(self, leakage_coefficient):\n",
    "        super().__init__()\n",
    "        self.leak_coef = leakage_coefficient\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.gradient = np.where(x > 0, 1.0, self.leak_coef)\n",
    "        return np.maximum(x, 0) + self.leak_coef * np.minimum(0, x)\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        assert self.gradient is not None, \"Must call forward before backward\"\n",
    "        return gradient * self.gradient\n",
    "\n",
    "\n",
    "# Softmax output for multiclass predictions\n",
    "class SoftmaxOutputLayer(NeuralNetLayer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cur_probs = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_small = x - np.max(x, axis=-1, keepdims=True)\n",
    "        exps = np.exp(x_small)\n",
    "        probs = exps / np.sum(exps, axis=-1)[:, None]\n",
    "        self.cur_probs = probs\n",
    "        return probs\n",
    "\n",
    "    def backward(self, target):\n",
    "        assert self.cur_probs is not None, \"Must call forward before backward\"\n",
    "        return self.cur_probs - target\n",
    "\n",
    "\n",
    "class MLP:\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features,\n",
    "        num_classes,\n",
    "        activation_function,\n",
    "        num_hidden_layers,\n",
    "        num_hidden_units,\n",
    "    ):  # option for activation function are ['relu', 'tanh', 'leaky']\n",
    "        assert activation_function in [\n",
    "            \"relu\",\n",
    "            \"tanh\",\n",
    "            \"leaky\",\n",
    "        ], \"options for activation function are ['relu', 'tanh', 'leaky'].\"\n",
    "        assert num_hidden_layers == len(\n",
    "            num_hidden_units\n",
    "        ), \"the length of the hidden units array must match the number of hidden layers\"\n",
    "\n",
    "        if num_hidden_layers == 0:\n",
    "            self.layers = [\n",
    "                LinearLayer(input_size=num_features, output_size=num_classes)\n",
    "            ]\n",
    "            self.layers.append(SoftmaxOutputLayer())\n",
    "        else:\n",
    "            # Initialize the input layer\n",
    "            self.layers = [LinearLayer(num_features, num_hidden_units[0])]\n",
    "            if activation_function == \"relu\":\n",
    "                self.layers.append(ReLULayer())\n",
    "            elif activation_function == \"tanh\":\n",
    "                self.layers.append(TanhLayer())\n",
    "            elif activation_function == \"leaky\":\n",
    "                self.layers.append(LeakyReLULayer(leakage_coefficient=0.001))\n",
    "\n",
    "            # Initialize each hidden layer\n",
    "            for i in range(0, num_hidden_layers - 1):\n",
    "                self.layers.append(\n",
    "                    LinearLayer(\n",
    "                        input_size=self.layers[i * 2].no,\n",
    "                        output_size=num_hidden_units[i],\n",
    "                    )\n",
    "                )\n",
    "                if activation_function == \"relu\":\n",
    "                    self.layers.append(ReLULayer())\n",
    "                elif activation_function == \"tanh\":\n",
    "                    self.layers.append(TanhLayer())\n",
    "                elif activation_function == \"leaky\":\n",
    "                    self.layers.append(LeakyReLULayer(leakage_coefficient=0.001))\n",
    "\n",
    "            # Initialize the output layer\n",
    "            self.layers.append(\n",
    "                LinearLayer(input_size=num_hidden_units[-1], output_size=num_classes)\n",
    "            )\n",
    "            self.layers.append(SoftmaxOutputLayer())\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, target):\n",
    "        for layer in self.layers[::-1]:\n",
    "            target = layer.backward(target)\n",
    "\n",
    "    def fit(self, X, y, optimizer, epochs=10000, mini_batch_size=10, verbose=True):\n",
    "        # No need to perform one-hot encoding since it's done within the dataset preprocessing\n",
    "\n",
    "        losses = []  # This now records the loss of each minibatch\n",
    "\n",
    "        # Shuffle the training dataset\n",
    "        shuffler = np.random.permutation(X.shape[0])\n",
    "        X = X[shuffler]\n",
    "        y = y[shuffler]\n",
    "\n",
    "        number_of_mini_batch = math.floor(X.shape[0] / mini_batch_size)\n",
    "        leftover = X.shape[0] % mini_batch_size\n",
    "\n",
    "        # batch_slice\n",
    "        for i in tqdm(range(epochs)):\n",
    "            losses_batch = []\n",
    "            for batch_index in range(number_of_mini_batch):\n",
    "                # Get a batch per slice\n",
    "                if batch_index != (number_of_mini_batch - 1):\n",
    "                    batch_slice = slice(\n",
    "                        batch_index * mini_batch_size,\n",
    "                        (batch_index + 1) * mini_batch_size,\n",
    "                    )\n",
    "                else:\n",
    "                    batch_slice = slice(\n",
    "                        batch_index * mini_batch_size,\n",
    "                        (batch_index + 1) * mini_batch_size + leftover,\n",
    "                    )\n",
    "\n",
    "                X_batch = X[batch_slice]\n",
    "                y_batch = y[batch_slice]\n",
    "\n",
    "                # Forward\n",
    "                epsilon = 1e-8\n",
    "                y_hat_batch = self.forward(X_batch)\n",
    "                y_hat_batch = y_hat_batch + epsilon\n",
    "                y_hat_batch[y_hat_batch > 1] = 1\n",
    "                loss_per_batch = -(y_batch * np.log(y_hat_batch)).sum(axis=-1).mean()\n",
    "                losses_batch.append(loss_per_batch)\n",
    "                # Backward\n",
    "                self.backward(y_batch)\n",
    "                optimizer.step()\n",
    "            loss_epoch_avg = np.mean(losses_batch)\n",
    "            losses.append(loss_epoch_avg)\n",
    "\n",
    "        plt.plot(losses)\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Cross entropy loss\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_hat = self.forward(X)\n",
    "        return y_hat\n",
    "\n",
    "\n",
    "class Optimizer:\n",
    "    def __init__(self, net: MLP):\n",
    "        self.net = net\n",
    "\n",
    "    def step(self):\n",
    "        for layer in self.net.layers[::-1]:\n",
    "            if layer.parameters is not None:\n",
    "                self.update(layer.parameters, layer.gradient)\n",
    "\n",
    "    def update(self, params, gradient):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class GradientDescentOptimizer(Optimizer):\n",
    "    def __init__(self, net: MLP, lr: float):\n",
    "        super().__init__(net)\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, params, gradient):\n",
    "        for p, g in zip(params, gradient):\n",
    "            p -= self.lr * g.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP class with numerical gradient checking\n",
    "class MLP_GradCheck:\n",
    "    def __init__(self,num_features, num_classes, activation_function, num_hidden_layers, num_hidden_units): # option for activation function are ['relu', 'tanh', 'leaky']\n",
    "        assert activation_function in ['relu', 'tanh', 'leaky'], 'options for activation function are [\\'relu\\', \\'tanh\\', \\'leaky\\'].'\n",
    "        assert num_hidden_layers == len(num_hidden_units), 'the length of the hidden units array must match the number of hidden layers'\n",
    "        \n",
    "        if num_hidden_layers == 0:\n",
    "            self.layers = [LinearLayer(input_size=num_features, output_size=num_classes)]\n",
    "            self.layers.append(SoftmaxOutputLayer())\n",
    "        else:\n",
    "            # # Initialize the input layer\n",
    "            # self.layers = [LinearLayer(num_features, num_hidden_units[0])]\n",
    "            # if activation_function == 'relu':\n",
    "            #     self.layers.append(ReLULayer())\n",
    "            # elif activation_function == 'tanh':\n",
    "            #     self.layers.append(TanhLayer())\n",
    "            # elif activation_function == 'leaky':\n",
    "            #     self.layers.append(LeakyReLULayer(leakage_coefficient=0.001))\n",
    "\n",
    "            # # Initialize each hidden layer\n",
    "            # for i in range(0, num_hidden_layers-1):\n",
    "            #     self.layers.append(LinearLayer(input_size=self.layers[i*2].no, output_size=num_hidden_units[i]))    \n",
    "            #     if activation_function == 'relu':\n",
    "            #         self.layers.append(ReLULayer())\n",
    "            #     elif activation_function == 'tanh':\n",
    "            #         self.layers.append(TanhLayer())\n",
    "            #     elif activation_function == 'leaky':\n",
    "            #         self.layers.append(LeakyReLULayer(leakage_coefficient=0.001))\n",
    "            \n",
    "            # # Initialize the output layer\n",
    "            # self.layers.append(LinearLayer(input_size=num_hidden_units[-1], output_size=num_classes))\n",
    "            # self.layers.append(SoftmaxOutputLayer())\n",
    "            \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, target):\n",
    "        for layer in self.layers[::-1]:\n",
    "            target = layer.backward(target)\n",
    "\n",
    "    def fit(self, X, y, optimizer, epochs=10000, mini_batch_size=10, verbose=True):\n",
    "        # No need to perform one-hot encoding since it's done within the dataset preprocessing\n",
    "          \n",
    "        losses = [] #This now records the loss of each minibatch\n",
    "\n",
    "        #Shuffle the training dataset\n",
    "        shuffler = np.random.permutation(X.shape[0])\n",
    "        X = X[shuffler]\n",
    "        y = y[shuffler]\n",
    "\n",
    "        number_of_mini_batch = math.floor(X.shape[0] / mini_batch_size)\n",
    "        leftover = X.shape[0] % mini_batch_size\n",
    "\n",
    "        # batch_slice\n",
    "        for i in tqdm(range(epochs)):\n",
    "          losses_batch = []\n",
    "          for batch_index in range(number_of_mini_batch):\n",
    "            # Get a batch per slice\n",
    "            if batch_index != (number_of_mini_batch-1):\n",
    "              batch_slice = slice(batch_index*mini_batch_size, (batch_index+1)*mini_batch_size)\n",
    "            else:\n",
    "              batch_slice = slice(batch_index*mini_batch_size, (batch_index+1)*mini_batch_size + leftover)\n",
    "            \n",
    "            X_batch = X[batch_slice]\n",
    "            y_batch = y[batch_slice]\n",
    "\n",
    "            #Forward\n",
    "            epsilon = 1e-8\n",
    "            y_hat_batch = self.forward(X_batch)\n",
    "            y_hat_batch = y_hat_batch + epsilon\n",
    "            y_hat_batch[y_hat_batch>1] = 1\n",
    "            loss_per_batch = -(y_batch * np.log(y_hat_batch)).sum(axis=-1).mean()\n",
    "            losses_batch.append(loss_per_batch)\n",
    "\n",
    "            #Backward\n",
    "            self.backward(y_batch)\n",
    "\n",
    "            # Gradient Check\n",
    "            eps = 1e-4\n",
    "            for i in range(len(self.layers)):\n",
    "              if self.layers[i].parameters is not None:\n",
    "                layer_gradients = [g for g in np.nditer(self.layers[i].gradient[0])]\n",
    "                for j, x in enumerate(np.nditer(self.layers[i].w, op_flags=['readwrite'])):\n",
    "                  x += eps\n",
    "                  y_hat_batch = self.forward(X_batch)\n",
    "                  y_hat_batch = y_hat_batch + epsilon\n",
    "                  y_hat_batch[y_hat_batch>1] = 1\n",
    "                  print(y_hat_batch)\n",
    "                  loss_per_batch_plus = -(y_batch * np.log(y_hat_batch)).sum(axis=-1).mean()\n",
    "\n",
    "                  x -= (2*eps)\n",
    "                  y_hat_batch = self.forward(X_batch)\n",
    "                  y_hat_batch = y_hat_batch + epsilon\n",
    "                  y_hat_batch[y_hat_batch>1] = 1\n",
    "                  print(y_hat_batch)\n",
    "                  loss_per_batch_minus = -(y_batch * np.log(y_hat_batch)).sum(axis=-1).mean()\n",
    "\n",
    "                  x += eps\n",
    "                  numerical_grad = (loss_per_batch_plus - loss_per_batch_minus) / (2 * eps)\n",
    "\n",
    "                  if not np.isclose(numerical_grad, layer_gradients[j], atol=0.0001):\n",
    "                    raise ValueError((f'Numerical gradient of {numerical_grad:.4f} is not close to the gradient of {layer_gradients[j]:.4f}.'))\n",
    "                    \n",
    "              print('No gradient errors.')\n",
    "\n",
    "            optimizer.step()\n",
    "          loss_epoch_avg = np.mean(losses_batch)\n",
    "          losses.append(loss_epoch_avg)\n",
    "        \n",
    "        plt.plot(losses)\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Cross entropy loss\")\n",
    "        \n",
    "    def predict(self, X):\n",
    "        y_hat = self.forward(X)\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layers with L2 regularization implementation\n",
    "class LinearLayer_L2(NeuralNetLayer):\n",
    "    def __init__(self, input_size, output_size, lmbda):\n",
    "        super().__init__()\n",
    "        self.ni = input_size\n",
    "        self.no = output_size\n",
    "        self.w = np.random.randn(output_size, input_size)\n",
    "        self.b = np.random.randn(output_size)\n",
    "        self.cur_input = None\n",
    "        self.parameters = [self.w, self.b]\n",
    "        self.lmbda = lmbda\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.cur_input = x\n",
    "        return (self.w[None, :, :] @ x[:, :, None]).squeeze() + self.b\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        assert self.cur_input is not None, \"Must call forward before backward\"\n",
    "        # dw = gradient.dot(self.cur_input)\n",
    "        dw = gradient[:, :, None] @ (self.cur_input[:, None, :])\n",
    "        dw += self.lmbda * self.w\n",
    "        db = gradient\n",
    "        self.gradient = [dw, db]\n",
    "        return gradient.dot(self.w)\n",
    "\n",
    "\n",
    "# Instatiation of MLP with L2 Regularization\n",
    "class MLP_L2:\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features,\n",
    "        num_classes,\n",
    "        activation_function,\n",
    "        num_hidden_layers,\n",
    "        num_hidden_units,\n",
    "        lmbda,\n",
    "    ):  # option for activation function are ['relu', 'tanh', 'leaky']\n",
    "        assert activation_function in [\n",
    "            \"relu\",\n",
    "            \"tanh\",\n",
    "            \"leaky\",\n",
    "        ], \"options for activation function are ['relu', 'tanh', 'leaky'].\"\n",
    "        assert num_hidden_layers == len(\n",
    "            num_hidden_units\n",
    "        ), \"the length of the hidden units array must match the number of hidden layers\"\n",
    "\n",
    "        if num_hidden_layers == 0:\n",
    "            self.layers = [\n",
    "                LinearLayer_L2(\n",
    "                    input_size=num_features, output_size=num_classes, lmbda=lmbda\n",
    "                )\n",
    "            ]\n",
    "            self.layers.append(SoftmaxOutputLayer())\n",
    "        else:\n",
    "            # # Initialize the input layer\n",
    "            # self.layers = [LinearLayer_L2(num_features, num_hidden_units[0, lmbda=lmbda])]\n",
    "            # if activation_function == 'relu':\n",
    "            #     self.layers.append(ReLULayer())\n",
    "            # elif activation_function == 'tanh':\n",
    "            #     self.layers.append(TanhLayer())\n",
    "            # elif activation_function == 'leaky':\n",
    "            #     self.layers.append(LeakyReLULayer(leakage_coefficient=0.001))\n",
    "\n",
    "            # # Initialize each hidden layer\n",
    "            # for i in range(0, num_hidden_layers-1):\n",
    "            #     self.layers.append(LinearLayer_L2(input_size=self.layers[i*2].no, output_size=num_hidden_units[i, lmbda=lmbda]))\n",
    "            #     if activation_function == 'relu':\n",
    "            #         self.layers.append(ReLULayer())\n",
    "            #     elif activation_function == 'tanh':\n",
    "            #         self.layers.append(TanhLayer())\n",
    "            #     elif activation_function == 'leaky':\n",
    "            #         self.layers.append(LeakyReLULayer(leakage_coefficient=0.001))\n",
    "\n",
    "            # # Initialize the output layer\n",
    "            # self.layers.append(LinearLayer_L2(input_size=num_hidden_units[-1], output_size=num_classes, lmbda=lmbda))\n",
    "            # self.layers.append(SoftmaxOutputLayer())\n",
    "\n",
    "            # Initialize the output layer\n",
    "            self.layers.append(\n",
    "                LinearLayer_L2(\n",
    "                    input_size=num_hidden_units[-1],\n",
    "                    output_size=num_classes,\n",
    "                    lmbda=lmbda,\n",
    "                )\n",
    "            )\n",
    "            self.layers.append(SoftmaxOutputLayer())\n",
    "        self.lmbda = lmbda\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, target):\n",
    "        for layer in self.layers[::-1]:\n",
    "            target = layer.backward(target)\n",
    "\n",
    "    def fit(self, X, y, optimizer, epochs=10000, mini_batch_size=10, verbose=True):\n",
    "        # No need to perform one-hot encoding since it's done within the dataset preprocessing\n",
    "\n",
    "        losses = []  # This now records the loss of each minibatch\n",
    "\n",
    "        # Shuffle the training dataset\n",
    "        shuffler = np.random.permutation(X.shape[0])\n",
    "        X = X[shuffler]\n",
    "        y = y[shuffler]\n",
    "\n",
    "        number_of_mini_batch = math.floor(X.shape[0] / mini_batch_size)\n",
    "        leftover = X.shape[0] % mini_batch_size\n",
    "\n",
    "        # batch_slice\n",
    "        for i in tqdm(range(epochs)):\n",
    "            losses_batch = []\n",
    "            for batch_index in range(number_of_mini_batch):\n",
    "                # Get a batch per slice\n",
    "                if batch_index != (number_of_mini_batch - 1):\n",
    "                    batch_slice = slice(\n",
    "                        batch_index * mini_batch_size,\n",
    "                        (batch_index + 1) * mini_batch_size,\n",
    "                    )\n",
    "                else:\n",
    "                    batch_slice = slice(\n",
    "                        batch_index * mini_batch_size,\n",
    "                        (batch_index + 1) * mini_batch_size + leftover,\n",
    "                    )\n",
    "\n",
    "                X_batch = X[batch_slice]\n",
    "                y_batch = y[batch_slice]\n",
    "\n",
    "                # Forward\n",
    "                epsilon = 1e-8\n",
    "                y_hat_batch = self.forward(X_batch)\n",
    "                y_hat_batch = y_hat_batch + epsilon\n",
    "                y_hat_batch[y_hat_batch > 1] = 1\n",
    "                s = (self.lmbda / 2) * np.mean(\n",
    "                    [\n",
    "                        np.square(self.layers[i * 2].w).sum()\n",
    "                        for i in range(math.floor(len(self.layers) / 2))\n",
    "                    ]\n",
    "                )\n",
    "                L = -(y_batch * np.log(y_hat_batch)).sum(axis=-1).mean()\n",
    "                loss_per_batch = L + s\n",
    "                losses_batch.append(loss_per_batch)\n",
    "                # Backward\n",
    "                self.backward(y_batch)\n",
    "                optimizer.step()\n",
    "            loss_epoch_avg = np.mean(losses_batch)\n",
    "            losses.append(loss_epoch_avg)\n",
    "\n",
    "        plt.plot(losses)\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Cross entropy loss\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_hat = self.forward(X)\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textual Data Preprocessing & Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labeled_bow_file = open(\"/content/aclImdb/train/labeledBow.feat\", \"r\")\n",
    "train_labeled_bow = {}\n",
    "train_total_appear_in_reviews = {}\n",
    "for line in train_labeled_bow_file.readlines():\n",
    "    for i, feature_index_occurence in enumerate(\n",
    "        line.strip().split(\" \")\n",
    "    ):  # split the values in each review into word index/number of occurences pairs\n",
    "        if i == 0:  # skip the rating as it is the first element in the split\n",
    "            continue\n",
    "        tmp_arr = feature_index_occurence.split(\":\")\n",
    "        word_index, word_occurence = (\n",
    "            int(tmp_arr[0]),\n",
    "            int(tmp_arr[1]),\n",
    "        )  # extract word index/word occurence\n",
    "        if (\n",
    "            not imdb_corpus[word_index] in train_labeled_bow\n",
    "        ):  # if word is not in dict, instantiate its review appearance at 1\n",
    "            train_labeled_bow[imdb_corpus[word_index]] = word_occurence\n",
    "            train_total_appear_in_reviews[imdb_corpus[word_index]] = 1\n",
    "        else:\n",
    "            train_labeled_bow[imdb_corpus[word_index]] += word_occurence\n",
    "            train_total_appear_in_reviews[imdb_corpus[word_index]] += (\n",
    "                1  # if it is already in dict, increment review appearance by 1\n",
    "            )\n",
    "\n",
    "train_labeled_rf = {\n",
    "    k: v / 25000.0 for k, v in train_total_appear_in_reviews.items()\n",
    "}  # contains the review frequencies for all words in the corpus\n",
    "train_labeled_idf = {\n",
    "    k: np.log(1 + (25000.0 / train_total_appear_in_reviews[k]))\n",
    "    for k, v in train_labeled_bow.items()\n",
    "}  # computes inverse document frequency to determine importance of words\n",
    "print(train_labeled_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find words that appear in less than 1% of the documents and words that appear in more than 50% of the documents\n",
    "filter_train = {}\n",
    "for key, value in train_labeled_rf.items():\n",
    "    if value > 0.01 and value < 0.5:\n",
    "        filter_train[key] = value\n",
    "\n",
    "print(\"Number of  features in the first train samples are:\", len(train_labeled_rf))\n",
    "print(\"Number of filtered features in train samples are:\", len(filter_train))\n",
    "# print(filter_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labeled_bow_file = open(\"/content/aclImdb/test/labeledBow.feat\", \"r\")\n",
    "test_labeled_bow = {}\n",
    "test_total_appear_in_reviews = {}\n",
    "for line in test_labeled_bow_file.readlines():\n",
    "    for i, feature_index_occurence in enumerate(\n",
    "        line.strip().split(\" \")\n",
    "    ):  # split the values in each review into word index/number of occurences pairs\n",
    "        if i == 0:  # skip the rating as it is the first element in the split\n",
    "            continue\n",
    "        tmp_arr = feature_index_occurence.split(\":\")\n",
    "        word_index, word_occurence = (\n",
    "            int(tmp_arr[0]),\n",
    "            int(tmp_arr[1]),\n",
    "        )  # extract word index/word occurence\n",
    "        if (\n",
    "            not imdb_corpus[word_index] in test_labeled_bow\n",
    "        ):  # if word is not in dict, instantiate its review appearance at 1\n",
    "            test_labeled_bow[imdb_corpus[word_index]] = word_occurence\n",
    "            test_total_appear_in_reviews[imdb_corpus[word_index]] = 1\n",
    "        else:\n",
    "            test_labeled_bow[imdb_corpus[word_index]] += word_occurence\n",
    "            test_total_appear_in_reviews[imdb_corpus[word_index]] += (\n",
    "                1  # if it is already in dict, increment review appearance by 1\n",
    "            )\n",
    "\n",
    "test_labeled_rf = {\n",
    "    k: v / 25000.0 for k, v in test_total_appear_in_reviews.items()\n",
    "}  # contains the review frequencies for all words in the corpus\n",
    "test_labeled_idf = {\n",
    "    k: np.log(1 + (25000.0 / test_total_appear_in_reviews[k]))\n",
    "    for k, v in test_labeled_bow.items()\n",
    "}  # computes inverse document frequency to determine importance of words\n",
    "# print(test_labeled_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find words that appear in less than 1% of the documents and words that appear in more than 50% of the documents\n",
    "filter_test = {}\n",
    "for key, value in test_labeled_rf.items():\n",
    "    if value > 0.01 and value < 0.5:\n",
    "        filter_test[key] = value\n",
    "\n",
    "print(\"Number of  features in the first test samples are:\", len(test_labeled_rf))\n",
    "print(\"Number of filtered features in test samples are:\", len(filter_test))\n",
    "# print(filter_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute top features based on their absolute z-scores associated with continuous ratings (1-10) and build the training dataset\n",
    "train_labeled_bow_file = open(\"/content/aclImdb/train/labeledBow.feat\", \"r\")\n",
    "filtered_words_train = (\n",
    "    filter_train.keys()\n",
    ")  # isolate only the words from the filtered dict\n",
    "\n",
    "# create an empty dataframe to store word occurence at each review\n",
    "train_review_words_occurence_df = pd.DataFrame(\n",
    "    np.zeros((25000, len(filtered_words_train))), columns=filtered_words_train\n",
    ")\n",
    "\n",
    "review_index = 0\n",
    "ratings = []\n",
    "for line in train_labeled_bow_file.readlines():\n",
    "    for i, feature_index_occurence in enumerate(line.strip().split(\" \")):\n",
    "        if i == 0:\n",
    "            rating = int(\n",
    "                feature_index_occurence\n",
    "            )  # store rating as first element in review info split\n",
    "            ratings.append(rating)\n",
    "            continue\n",
    "        tmp_arr = feature_index_occurence.split(\":\")\n",
    "        word_index, word_occurence = int(tmp_arr[0]), int(tmp_arr[1])\n",
    "        if imdb_corpus[word_index] in filtered_words_train:\n",
    "            train_review_words_occurence_df.at[\n",
    "                review_index, imdb_corpus[word_index]\n",
    "            ] = word_occurence  # store the overall word occurence for the word at that rating\n",
    "    review_index += 1\n",
    "train_review_words_occurence = train_review_words_occurence_df.to_numpy()\n",
    "display(train_review_words_occurence_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rating_words_standardized = np.zeros((25000, len(filtered_words_train)))\n",
    "\n",
    "# standardize each word (using absolute value) at each review\n",
    "for feature in range(\n",
    "    train_rating_words_standardized.shape[1]\n",
    "):  # loop through each word (col) of rating/word frequency df\n",
    "    feature_mean = np.mean(train_review_words_occurence[:, feature])  # mean for column\n",
    "    feature_sd = np.std(\n",
    "        train_review_words_occurence[:, feature] - feature_mean\n",
    "    )  # calculate standard deviation over the mean of each word\n",
    "    for instance in range(train_rating_words_standardized.shape[0]):\n",
    "        train_rating_words_standardized[instance, feature] = (\n",
    "            train_review_words_occurence[instance, feature] - feature_mean\n",
    "        ) / feature_sd\n",
    "\n",
    "train_rating_words_standardized_df = pd.DataFrame(\n",
    "    train_rating_words_standardized, columns=filtered_words_train\n",
    ")\n",
    "# display(train_rating_words_standardized_df)\n",
    "ratings_standardized = (np.asarray(ratings) - np.std(ratings)) / np.mean(ratings)\n",
    "# print(ratings_standardized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_scores = np.abs(\n",
    "    np.dot(train_rating_words_standardized.T, ratings_standardized) / np.sqrt(25000)\n",
    ")  # compute z-score list from Hypothesis Testing slides\n",
    "\n",
    "words_zscores = {}\n",
    "index = 0\n",
    "for word in (\n",
    "    train_rating_words_standardized_df\n",
    "):  # associate each word with its newly calculated, total z-score\n",
    "    words_zscores[word] = z_scores[index]\n",
    "    index += 1\n",
    "\n",
    "words_zscores_sorted_reverse = sorted(\n",
    "    words_zscores, key=words_zscores.get, reverse=True\n",
    ")  # sort z-scores by least to maximum\n",
    "words_zscores_sorted_reverse = words_zscores_sorted_reverse[:1000]\n",
    "important_features = {}\n",
    "\n",
    "for word in (\n",
    "    words_zscores_sorted_reverse\n",
    "):  # populate new dict with words that have greatest z-scores\n",
    "    important_features[word] = words_zscores[word]\n",
    "\n",
    "important_features_index = {}\n",
    "for important_word in important_features.keys():\n",
    "    for i, word in enumerate(imdb_corpus):\n",
    "        if word == important_word:\n",
    "            important_features_index[i] = words_zscores[word]\n",
    "print(important_features)\n",
    "print(important_features_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out only the important words from each review\n",
    "train_labeled_bow_file = open(\"/content/aclImdb/train/labeledBow.feat\", \"r\")\n",
    "\n",
    "# create an empty dataframe to store word occurence at each review\n",
    "X_train = pd.DataFrame(\n",
    "    np.zeros((25000, len(important_features_index))), columns=important_features_index\n",
    ")\n",
    "\n",
    "review_index = 0\n",
    "y_train = []\n",
    "for line in train_labeled_bow_file.readlines():\n",
    "    for i, feature_index_occurence in enumerate(line.strip().split(\" \")):\n",
    "        if i == 0:\n",
    "            rating = int(\n",
    "                feature_index_occurence\n",
    "            )  # store rating as first element in review info split\n",
    "            if rating >= 5:\n",
    "                rating = 1\n",
    "            else:\n",
    "                rating = 0\n",
    "            y_train.append(rating)\n",
    "            continue\n",
    "        tmp_arr = feature_index_occurence.split(\":\")\n",
    "        word_index, word_occurence = int(tmp_arr[0]), int(tmp_arr[1])\n",
    "        if word_index in important_features_index.keys():\n",
    "            X_train.at[review_index, word_index] = (\n",
    "                word_occurence  # store the overall word occurence for the word at that rating\n",
    "            )\n",
    "    review_index += 1\n",
    "X_train = X_train.to_numpy()\n",
    "y_train = np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the data matrix and label vector for the testing dataset\n",
    "# filter out only the important words from each review\n",
    "test_labeled_bow_file = open(\"/content/aclImdb/test/labeledBow.feat\", \"r\")\n",
    "\n",
    "# create an empty dataframe to store word occurence at each review\n",
    "X_test = pd.DataFrame(\n",
    "    np.zeros((25000, len(important_features_index))), columns=important_features_index\n",
    ")\n",
    "\n",
    "review_index = 0\n",
    "y_test = []\n",
    "for line in test_labeled_bow_file.readlines():\n",
    "    for i, feature_index_occurence in enumerate(line.strip().split(\" \")):\n",
    "        if i == 0:\n",
    "            rating = int(\n",
    "                feature_index_occurence\n",
    "            )  # store rating as first element in review info split\n",
    "            if rating >= 5:  # no positive rating is less than 5\n",
    "                rating = 1\n",
    "            else:  # no negative rating is greater than 5\n",
    "                rating = 0\n",
    "            y_test.append(rating)\n",
    "            continue\n",
    "        tmp_arr = feature_index_occurence.split(\":\")\n",
    "        word_index, word_occurence = int(tmp_arr[0]), int(tmp_arr[1])\n",
    "        if word_index in important_features_index.keys():\n",
    "            X_test.at[review_index, word_index] = (\n",
    "                word_occurence  # store the overall word occurence for the word at that rating\n",
    "            )\n",
    "    review_index += 1\n",
    "X_test = X_test.to_numpy()\n",
    "y_test = np.asarray(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar Plot of Top 10 Postive and Negative Words from Simple Linear Regression Hypothesis Testing\n",
    "\n",
    "# standardize each word (using absolute value) at each review\n",
    "# we leave this version with no absolute value so that the question posed at the end of Task 1.1 can be answered.\n",
    "train_rating_words_standardized = np.zeros((25000, len(filtered_words_train)))\n",
    "\n",
    "for feature in range(\n",
    "    train_rating_words_standardized.shape[1]\n",
    "):  # loop through each word (col) of rating/word frequency df\n",
    "    feature_mean = np.mean(train_review_words_occurence[:, feature])\n",
    "    feature_sd = np.std(train_review_words_occurence[:, feature])\n",
    "    for instance in range(train_rating_words_standardized.shape[0]):\n",
    "        train_rating_words_standardized[instance, feature] = (\n",
    "            train_review_words_occurence[instance, feature] - feature_mean\n",
    "        ) / feature_sd\n",
    "\n",
    "train_rating_words_standardized_df = pd.DataFrame(\n",
    "    train_rating_words_standardized, columns=filtered_words_train\n",
    ")\n",
    "# display(train_rating_words_standardized_df)\n",
    "ratings_standardized = [\n",
    "    (r - np.std(ratings)) / np.mean(ratings) for r in ratings\n",
    "]  # CONVERT TO NUMPY ARRAY\n",
    "# print(ratings_standardized)\n",
    "\n",
    "z_scores = np.dot(train_rating_words_standardized.T, ratings_standardized) / np.sqrt(\n",
    "    25000\n",
    ")  # compute z-score list from Hypothesis Testing slides\n",
    "words_zscores = {}\n",
    "index = 0\n",
    "\n",
    "for word in (\n",
    "    train_rating_words_standardized_df\n",
    "):  # associate each word with its newly calculated, total z-score\n",
    "    words_zscores[word] = z_scores[index]\n",
    "    index += 1\n",
    "\n",
    "words_zscores_sorted_reversed_list = sorted(\n",
    "    words_zscores, key=words_zscores.get, reverse=True\n",
    ")  # sort z-scores by least to maximum\n",
    "words_zscores_sorted_reversed = {}\n",
    "for word in (\n",
    "    words_zscores_sorted_reversed_list\n",
    "):  # populate new dict with words that have greatest z-scores\n",
    "    words_zscores_sorted_reversed[word] = words_zscores[word]\n",
    "print(words_zscores_sorted_reversed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_positive_x = []\n",
    "top_10_positive_y = []\n",
    "top_10_negative_x = []\n",
    "top_10_negative_y = []\n",
    "\n",
    "index = 0\n",
    "for word, z_score in words_zscores_sorted_reversed.items():\n",
    "    if index >= 0 and index <= 9:\n",
    "        top_10_positive_x.append(z_score)\n",
    "        top_10_positive_y.append(word)\n",
    "    elif index >= (len(words_zscores_sorted_reversed.keys()) - 10) and index <= (\n",
    "        len(words_zscores_sorted_reversed.keys()) - 1\n",
    "    ):\n",
    "        top_10_negative_x.append(z_score)\n",
    "        top_10_negative_y.append(word)\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.barh(top_10_positive_y, top_10_positive_x)\n",
    "plt.barh(top_10_negative_y, top_10_negative_x)\n",
    "plt.title(\"Top 10 Positive And Negative Words With z-scores\")\n",
    "plt.ylabel(\"word\")\n",
    "plt.xlabel(\"z-score\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
